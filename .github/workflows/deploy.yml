name: AWS Infrastructure Deployment

on:
  push:
    branches:
      - main
    paths-ignore:
      - '**.md'
      - 'docs/**'
      - '.github/ISSUE_TEMPLATE/**'
      - 'LICENSE'
      - '**/*.png'
      - '**/*.jpg'
  pull_request:
    branches:
      - develop
    paths-ignore:
      - '**.md'
      - 'docs/**'
      - '.github/ISSUE_TEMPLATE/**'
      - 'LICENSE'
      - '**/*.png'
      - '**/*.jpg'
  workflow_dispatch:

permissions:
  id-token: write
  contents: read

env:
  TF_LOG: INFO
  AWS_REGION: us-east-2
  ECR_REPOSITORY: nomadcrew
  TF_VERSION: 1.5.7
  ENVIRONMENT: ${{ github.event_name == 'pull_request' || github.ref == 'refs/heads/develop' ? 'staging' : 'production' }}

jobs:
  validate:
    name: Validate Terraform
    runs-on: ubuntu-latest
    outputs:
      fmt_outcome: ${{ steps.fmt.outcome }}
      init_outcome: ${{ steps.init.outcome }}
    steps:
      - uses: actions/checkout@v3
        name: Checkout code

      - uses: hashicorp/setup-terraform@v2
        name: Setup Terraform
        with:
          terraform_version: ${{ env.TF_VERSION }}
          cli_config_credentials_token: ${{ secrets.TF_API_TOKEN }}

      - uses: actions/cache@v3
        name: Cache Terraform plugins
        with:
          path: ~/.terraform.d/plugin-cache
          key: ${{ runner.os }}-terraform-${{ hashFiles('**/.terraform.lock.hcl') }}
          restore-keys: |
            ${{ runner.os }}-terraform-

      - id: fmt
        name: Terraform Format
        run: terraform fmt -check -recursive

      - uses: aws-actions/configure-aws-credentials@v4
        name: Configure AWS Credentials
        with:
          role-to-assume: ${{ github.event_name == 'pull_request' ? 'arn:aws:iam::195275657852:role/NomadCrewStagingDeploymentRole' : (env.ENVIRONMENT == 'production' ? 'arn:aws:iam::195275657852:role/NomadCrewProductionDeploymentRole' : 'arn:aws:iam::195275657852:role/NomadCrewStagingDeploymentRole') }}
          aws-region: ${{ env.AWS_REGION }}

      - id: init
        name: Terraform Init
        working-directory: terraform  # Corrected working directory
        run: terraform init -input=false

      - id: validate
        name: Terraform Validate
        working-directory: terraform  # Corrected working directory
        run: terraform validate -no-color

  security-scan:
    name: Security Scan
    runs-on: ubuntu-latest
    needs: [] # Run in parallel with validate
    steps:
      - uses: actions/checkout@v3
        name: Checkout code

      - uses: aquasecurity/tfsec-action@v1.0.0
        name: Run tfsec
        with:
          working_directory: terraform
          format: sarif
          soft_fail: true

      - uses: github/codeql-action/upload-sarif@v2
        name: Upload SARIF file
        with:
          sarif_file: tfsec.sarif

  plan:
    name: Plan Terraform Changes
    needs: [validate, security-scan]
    runs-on: ubuntu-latest
    outputs:
      has_changes: ${{ steps.plan.outputs.has_changes }}
    steps:
      - uses: actions/checkout@v3
        name: Checkout code

      - uses: hashicorp/setup-terraform@v2
        name: Setup Terraform
        with:
          terraform_version: ${{ env.TF_VERSION }}

      - uses: actions/cache@v3
        name: Cache Terraform plugins
        with:
          path: ~/.terraform.d/plugin-cache
          key: ${{ runner.os }}-terraform-${{ hashFiles('**/.terraform.lock.hcl') }}
          restore-keys: |
            ${{ runner.os }}-terraform-

      - uses: aws-actions/configure-aws-credentials@v4
        name: Configure AWS Credentials
        with:
          role-to-assume: ${{ env.ENVIRONMENT == 'production' ? 'arn:aws:iam::195275657852:role/NomadCrewProductionDeploymentRole' : 'arn:aws:iam::195275657852:role/NomadCrewStagingDeploymentRole' }}
          aws-region: ${{ env.AWS_REGION }}

      - id: init
        name: Terraform Init
        working-directory: terraform # Corrected working directory
        run: terraform init -input=false

      - name: Set Terraform Workspace
        working-directory: terraform # Corrected working directory
        run: |
          terraform workspace select ${{ env.ENVIRONMENT }} || terraform workspace new ${{ env.ENVIRONMENT }}

      - id: plan
        name: Terraform Plan
        working-directory: terraform # Corrected working directory
        continue-on-error: true # Allow plan to continue even if there are no changes. This is crucial for the has_changes output to work correctly.
        run: |
          terraform plan -no-color -refresh=false -var="environment=${{ env.ENVIRONMENT }}" \
            -var="supabase_anon_key=${{ secrets.SUPABASE_ANON_KEY }}" \
            -var="supabase_url=${{ secrets.SUPABASE_URL }}" \
            -var="google_web_client_id=${{ secrets.GOOGLE_WEB_CLIENT_ID }}" \
            -var="google_places_api_key=${{ secrets.GOOGLE_PLACES_API_KEY }}" \
            -var="db_password=${{ secrets.DB_PASSWORD }}" \
            -var="redis_password=${{ secrets.REDIS_PASSWORD }}" \
            -var="jwt_secret_key=${{ secrets.JWT_SECRET_KEY }}" -out=plan.txt

          echo "plan_output<<EOF" >> $GITHUB_OUTPUT
          cat plan.txt >> $GITHUB_OUTPUT
          echo "EOF" >> $GITHUB_OUTPUT

          if grep -q "No changes. Infrastructure is up-to-date." plan.txt; then # More robust check for no changes
            echo "::set-output name=has_changes::false"
          else
            echo "::set-output name=has_changes::true"
          fi


      - uses: actions/github-script@v6
        if: github.event_name == 'pull_request'
        name: Comment Plan
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const output = `#### Terraform Format and Style üñå\`${{ steps.fmt.outcome }}\`
            #### Terraform Initialization ‚öôÔ∏è\`${{ steps.init.outcome }}\`
            #### Terraform Plan üìñ\`${{ steps.plan.outcome }}\`
            <details><summary>Show Plan</summary>

            \`\`\`terraform
            ${{ steps.plan.outputs.plan_output }}
            \`\`\`

            </details>

            *Pushed by: @${{ github.actor }}, Action: \`${{ github.event_name }}\`*`;

            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: output
            })

  build-and-push:
    name: Build and Push Docker Image
    runs-on: ubuntu-latest
    needs: [] # No need to wait for plan if we're doing a push/dispatch
    if: github.event_name == 'push' || github.event_name == 'workflow_dispatch'
    outputs:
      image_tag: ${{ steps.build-image.outputs.image_tag }}
      registry_url: ${{ steps.login-ecr.outputs.registry }}
    steps:
      - uses: actions/checkout@v3
        name: Checkout code

      - uses: aws-actions/configure-aws-credentials@v4
        name: Configure AWS credentials
        with:
          role-to-assume: ${{ env.ENVIRONMENT == 'production' ? 'arn:aws:iam::195275657852:role/NomadCrewProductionDeploymentRole' : 'arn:aws:iam::195275657852:role/NomadCrewStagingDeploymentRole' }}
          aws-region: ${{ env.AWS_REGION }}


      - id: login-ecr
        name: Login to Amazon ECR
        uses: aws-actions/amazon-ecr-login@v1

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v2

      - id: build-image # Corrected duplicate ID
        name: Build and push Docker image
        uses: docker/build-push-action@v4
        with:
          context: .
          push: true
          tags: ${{ steps.login-ecr.outputs.registry }}/${{ env.ECR_REPOSITORY }}:${{ github.sha }},${{ steps.login-ecr.outputs.registry }}/${{ env.ECR_REPOSITORY }}:latest
          cache-from: type=gha
          cache-to: type=gha,mode=max
          build-args: |
            SERVER_ENVIRONMENT=${{ env.ENVIRONMENT == 'production' ? 'production' : 'development' }}
            JWT_SECRET_KEY=${{ secrets.JWT_SECRET_KEY }}
            DB_PASSWORD=${{ secrets.DB_PASSWORD }}
            REDIS_PASSWORD=${{ secrets.REDIS_PASSWORD }}
            RESEND_API_KEY=${{ secrets.RESEND_API_KEY }}
            GEOAPIFY_KEY=${{ secrets.GEOAPIFY_KEY }}
            PEXELS_API_KEY=${{ secrets.PEXELS_API_KEY }}
            SUPABASE_ANON_KEY=${{ secrets.SUPABASE_ANON_KEY }}
            SUPABASE_SERVICE_KEY=${{ secrets.SUPABASE_SERVICE_KEY }}
            SUPABASE_URL=${{ secrets.SUPABASE_URL }}
            SUPABASE_JWT_SECRET=${{ secrets.SUPABASE_JWT_SECRET }}
            EMAIL_FROM_ADDRESS=${{ secrets.EMAIL_FROM_ADDRESS }}
            EMAIL_FROM_NAME=${{ secrets.EMAIL_FROM_NAME }}
            FRONTEND_URL=${{ secrets.FRONTEND_URL }}
            ALLOWED_ORIGINS=${{ secrets.ALLOWED_ORIGINS }}

      - id: set-image-tag-output # Corrected duplicate ID, more descriptive name
        name: Set image tag output
        run: echo "::set-output name=image_tag::${{ github.sha }}"


  deploy:
    name: Deploy Infrastructure
    needs: [plan, build-and-push]
    if: (github.event_name == 'push' || github.event_name == 'workflow_dispatch')
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
        name: Checkout code

      - uses: aws-actions/configure-aws-credentials@v4
        name: Configure AWS Credentials
        with:
          role-to-assume: ${{ env.ENVIRONMENT == 'production' ? 'arn:aws:iam::195275657852:role/NomadCrewProductionDeploymentRole' : 'arn:aws:iam::195275657852:role/NomadCrewStagingDeploymentRole' }}
          aws-region: ${{ env.AWS_REGION }}
          role-duration-seconds: 3600

      - uses: hashicorp/setup-terraform@v2
        name: Setup Terraform
        with:
          terraform_version: ${{ env.TF_VERSION }}

      - uses: actions/cache@v3
        name: Cache Terraform plugins
        with:
          path: ~/.terraform.d/plugin-cache
          key: ${{ runner.os }}-terraform-${{ hashFiles('**/.terraform.lock.hcl') }}
          restore-keys: |
            ${{ runner.os }}-terraform-

      - name: Terraform Init
        working-directory: terraform # Corrected working directory
        run: terraform init -input=false

      - name: Set Terraform Workspace
        working-directory: terraform # Corrected working directory
        run: |
          terraform workspace select ${{ env.ENVIRONMENT }} || terraform workspace new ${{ env.ENVIRONMENT }}

      - id: check-changes
        name: Check for infrastructure changes
        working-directory: terraform # Corrected working directory
        run: |
          terraform plan -detailed-exitcode -var="environment=${{ env.ENVIRONMENT }}" \
            -var="supabase_anon_key=${{ secrets.SUPABASE_ANON_KEY }}" \
            -var="supabase_url=${{ secrets.SUPABASE_URL }}" \
            -var="google_web_client_id=${{ secrets.GOOGLE_WEB_CLIENT_ID }}" \
            -var="google_places_api_key=${{ secrets.GOOGLE_PLACES_API_KEY }}" \
            -var="db_password=${{ secrets.DB_PASSWORD }}" \
            -var="redis_password=${{ secrets.REDIS_PASSWORD }}" \
            -var="jwt_secret_key=${{ secrets.JWT_SECRET_KEY }}" \
            -var="docker_image=${{ needs.build-and-push.outputs.image_tag }}" > /dev/null
          if [[ $? -ne 0 ]]; then
            echo "::set-output name=has_changes::true"
          fi

      - name: Terraform Apply
        if: steps.check-changes.outputs.has_changes == 'true'
        working-directory: terraform # Corrected working directory
        run: |
          terraform apply -auto-approve \
            -var="environment=${{ env.ENVIRONMENT }}" \
            -var="supabase_anon_key=${{ secrets.SUPABASE_ANON_KEY }}" \
            -var="supabase_url=${{ secrets.SUPABASE_URL }}" \
            -var="google_web_client_id=${{ secrets.GOOGLE_WEB_CLIENT_ID }}" \
            -var="google_places_api_key=${{ secrets.GOOGLE_PLACES_API_KEY }}" \
            -var="db_password=${{ secrets.DB_PASSWORD }}" \
            -var="redis_password=${{ secrets.REDIS_PASSWORD }}" \
            -var="jwt_secret_key=${{ secrets.JWT_SECRET_KEY }}" \
            -var="docker_image=${{ needs.build-and-push.outputs.image_tag }}"

      - id: login-ecr-deploy # Added id and more descriptive name
        name: Login to Amazon ECR for Deploy
        uses: aws-actions/amazon-ecr-login@v1

      - name: Configure kubectl
        run: |
          aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name nomad-crew-cluster-${{ env.ENVIRONMENT }}

      - name: Deploy to Kubernetes
        run: |
          kubectl set image deployment/nomad-crew-frontend nomad-crew-frontend=${{ needs.build-and-push.outputs.registry_url }}/${{ env.ECR_REPOSITORY }}:${{ needs.build-and-push.outputs.image_tag }} --record
          kubectl rollout status deployment/nomad-crew-frontend

      - name: Set up rollback capability
        run: |
          CURRENT_REVISION=$(kubectl get deployment/nomad-crew-frontend -o jsonpath='{.metadata.annotations.deployment\.kubernetes\.io/revision}')
          echo "CURRENT_REVISION=$CURRENT_REVISION" >> $GITHUB_ENV

  verify-deployment:
    name: Verify Deployment
    needs: [deploy, build-and-push]
    if: github.event_name == 'push' || github.event_name == 'workflow_dispatch'
    runs-on: ubuntu-latest

    steps:
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ env.ENVIRONMENT == 'production' ? 'arn:aws:iam::195275657852:role/NomadCrewProductionDeploymentRole' : 'arn:aws:iam::195275657852:role/NomadCrewStagingDeploymentRole' }}
          aws-region: ${{ env.AWS_REGION }}

      - id: health-check
        name: Check application health
        run: |
          ALB_DNS=$(aws elbv2 describe-load-balancers --query "LoadBalancers[?contains(LoadBalancerName, 'nomadcrew')].DNSName" --output text)

          echo "Waiting for application to be healthy at https://$ALB_DNS/health/liveness"

          for i in {1..10}; do
            HTTP_STATUS=$(curl -s -o /dev/null -w "%{http_code}" https://$ALB_DNS/health/liveness)

            if [ "$HTTP_STATUS" -eq 200 ]; then
              echo "Application is healthy!"
              exit 0
            else
              echo "Attempt $i: Application is not healthy yet. Status: $HTTP_STATUS"
              sleep 30
            fi
          done

          echo "Application failed to become healthy after multiple attempts."
          exit 1

      - name: Rollback on failure
        if: failure() && steps.health-check.outcome == 'failure'
        run: |
          aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name nomad-crew-cluster-${{ env.ENVIRONMENT }}

          echo "Health check failed, rolling back deployment..."
          kubectl rollout undo deployment/nomad-crew-frontend
          kubectl rollout status deployment/nomad-crew-frontend

          echo "Rollback completed. Please check the application logs for more details."