name: AWS Infrastructure Deployment - Production

on:
  push:
    branches: [main]
    paths-ignore:
      - '**.md'
      - 'docs/**'
      - '.github/ISSUE_TEMPLATE/**'
      - 'LICENSE'
      - '**/*.{png,jpg}'
  workflow_dispatch:

permissions:
  id-token: write
  contents: read

env:
  TF_LOG: INFO
  AWS_REGION: us-east-2
  ECR_REPOSITORY: nomadcrew
  TF_VERSION: 1.5.7
  ENVIRONMENT: production
  TERRAFORM_DIR: deployment/terraform 

jobs:
  validate:
    name: Validate Terraform
    runs-on: ubuntu-latest
    outputs:
      fmt_outcome: ${{ steps.fmt.outcome }}
      init_outcome: ${{ steps.init.outcome }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v2
        with:
          terraform_version: ${{ env.TF_VERSION }}
          cli_config_credentials_token: ${{ secrets.TF_API_TOKEN }}

      - id: fmt
        name: Terraform Format
        run: terraform fmt -check -recursive

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: arn:aws:iam::195275657852:role/NomadCrewProductionDeploymentRole
          aws-region: ${{ env.AWS_REGION }}

      - id: init
        name: Terraform Init
        working-directory: ${{ env.TERRAFORM_DIR }}
        run: terraform init -input=false

      - id: validate
        name: Terraform Validate
        working-directory: ${{ env.TERRAFORM_DIR }}
        run: terraform validate -no-color

  security-scan:
    name: Security Scan
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Run tfsec
        uses: aquasecurity/tfsec-action@v1.0.0
        with:
          working_directory: ${{ env.TERRAFORM_DIR }}
          format: sarif
          soft_fail: true

      - name: Upload SARIF file
        uses: github/codeql-action/upload-sarif@v3
        with: 
          sarif_file: ${{ env.TERRAFORM_DIR }}/tfsec.sarif

  plan:
    name: Plan Terraform Changes
    needs: [validate, security-scan]
    runs-on: ubuntu-latest
    outputs:
      has_changes: ${{ steps.plan.outputs.has_changes }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v2
        with:
          terraform_version: ${{ env.TF_VERSION }}

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: arn:aws:iam::195275657852:role/NomadCrewProductionDeploymentRole
          aws-region: ${{ env.AWS_REGION }}

      - id: init
        name: Terraform Init
        working-directory: ${{ env.TERRAFORM_DIR }}
        run: terraform init -input=false

      - name: Set Terraform Workspace
        working-directory: ${{ env.TERRAFORM_DIR }}
        run: |
          terraform workspace select production || terraform workspace new production

      - id: plan
        name: Terraform Plan
        working-directory: ${{ env.TERRAFORM_DIR }}
        continue-on-error: true
        run: |
          terraform plan -no-color -refresh=false -var="environment=production" \
            -var="supabase_anon_key=${{ secrets.SUPABASE_ANON_KEY }}" \
            -var="supabase_url=${{ secrets.SUPABASE_URL }}" \
            -var="google_web_client_id=${{ secrets.GOOGLE_WEB_CLIENT_ID }}" \
            -var="google_places_api_key=${{ secrets.GOOGLE_PLACES_API_KEY }}" \
            -var="db_password=${{ secrets.DB_PASSWORD }}" \
            -var="redis_password=${{ secrets.REDIS_PASSWORD }}" \
            -var="jwt_secret_key=${{ secrets.JWT_SECRET_KEY }}" -out=plan.txt

          # Save plan output for PR comment
          echo "plan_output<<EOF" >> $GITHUB_OUTPUT
          cat plan.txt >> $GITHUB_OUTPUT
          echo "EOF" >> $GITHUB_OUTPUT

          # Determine if there are changes
          if grep -q "No changes. Infrastructure is up-to-date." plan.txt; then
            echo "has_changes=false" >> $GITHUB_OUTPUT
          else
            echo "has_changes=true" >> $GITHUB_OUTPUT
          fi

  build-and-push:
    name: Build and Push Docker Image
    runs-on: ubuntu-latest
    outputs:
      image_tag: ${{ steps.set-image-tag.outputs.image_tag }}
      registry_url: ${{ steps.login-ecr.outputs.registry }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: arn:aws:iam::195275657852:role/NomadCrewProductionDeploymentRole
          aws-region: ${{ env.AWS_REGION }}

      - id: login-ecr
        name: Login to Amazon ECR
        uses: aws-actions/amazon-ecr-login@v1

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v2

      - name: Build and push Docker image
        uses: docker/build-push-action@v4
        with:
          context: .
          push: true
          tags: ${{ steps.login-ecr.outputs.registry }}/${{ env.ECR_REPOSITORY }}:${{ github.sha }},${{ steps.login-ecr.outputs.registry }}/${{ env.ECR_REPOSITORY }}:latest
          cache-from: type=gha
          cache-to: type=gha,mode=max
          build-args: |
            SERVER_ENVIRONMENT=production
            JWT_SECRET_KEY=${{ secrets.JWT_SECRET_KEY }}
            DB_PASSWORD=${{ secrets.DB_PASSWORD }}
            REDIS_PASSWORD=${{ secrets.REDIS_PASSWORD }}
            RESEND_API_KEY=${{ secrets.RESEND_API_KEY }}
            GEOAPIFY_KEY=${{ secrets.GEOAPIFY_KEY }}
            PEXELS_API_KEY=${{ secrets.PEXELS_API_KEY }}
            SUPABASE_ANON_KEY=${{ secrets.SUPABASE_ANON_KEY }}
            SUPABASE_SERVICE_KEY=${{ secrets.SUPABASE_SERVICE_KEY }}
            SUPABASE_URL=${{ secrets.SUPABASE_URL }}
            SUPABASE_JWT_SECRET=${{ secrets.SUPABASE_JWT_SECRET }}
            EMAIL_FROM_ADDRESS=${{ secrets.EMAIL_FROM_ADDRESS }}
            EMAIL_FROM_NAME=${{ secrets.EMAIL_FROM_NAME }}
            FRONTEND_URL=${{ secrets.FRONTEND_URL }}
            ALLOWED_ORIGINS=${{ secrets.ALLOWED_ORIGINS }}

      - id: set-image-tag
        name: Set image tag output
        run: echo "image_tag=${{ github.sha }}" >> $GITHUB_OUTPUT

  deploy:
    name: Deploy Infrastructure
    needs: [plan, build-and-push]
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: arn:aws:iam::195275657852:role/NomadCrewProductionDeploymentRole
          aws-region: ${{ env.AWS_REGION }}
          role-duration-seconds: 3600

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v2
        with:
          terraform_version: ${{ env.TF_VERSION }}

      - name: Terraform Init
        working-directory: ${{ env.TERRAFORM_DIR }}
        run: terraform init -input=false

      - name: Set Terraform Workspace
        working-directory: ${{ env.TERRAFORM_DIR }}
        run: |
          terraform workspace select production || terraform workspace new production

      - id: check-changes
        name: Check for infrastructure changes
        working-directory: ${{ env.TERRAFORM_DIR }}
        run: |
          terraform plan -detailed-exitcode \
            -var="environment=production" \
            -var="supabase_anon_key=${{ secrets.SUPABASE_ANON_KEY }}" \
            -var="supabase_url=${{ secrets.SUPABASE_URL }}" \
            -var="google_web_client_id=${{ secrets.GOOGLE_WEB_CLIENT_ID }}" \
            -var="google_places_api_key=${{ secrets.GOOGLE_PLACES_API_KEY }}" \
            -var="db_password=${{ secrets.DB_PASSWORD }}" \
            -var="redis_password=${{ secrets.REDIS_PASSWORD }}" \
            -var="jwt_secret_key=${{ secrets.JWT_SECRET_KEY }}" \
            -var="docker_image=${{ needs.build-and-push.outputs.image_tag }}" > /dev/null || echo "has_changes=true" >> $GITHUB_OUTPUT

      - name: Terraform Apply
        if: steps.check-changes.outputs.has_changes == 'true'
        working-directory: ${{ env.TERRAFORM_DIR }}
        run: |
          terraform apply -auto-approve \
            -var="environment=production" \
            -var="supabase_anon_key=${{ secrets.SUPABASE_ANON_KEY }}" \
            -var="supabase_url=${{ secrets.SUPABASE_URL }}" \
            -var="google_web_client_id=${{ secrets.GOOGLE_WEB_CLIENT_ID }}" \
            -var="google_places_api_key=${{ secrets.GOOGLE_PLACES_API_KEY }}" \
            -var="db_password=${{ secrets.DB_PASSWORD }}" \
            -var="redis_password=${{ secrets.REDIS_PASSWORD }}" \
            -var="jwt_secret_key=${{ secrets.JWT_SECRET_KEY }}" \
            -var="docker_image=${{ needs.build-and-push.outputs.image_tag }}"

      - name: Login to Amazon ECR for Deploy
        id: login-ecr-deploy
        uses: aws-actions/amazon-ecr-login@v1

      - name: Configure kubectl
        run: |
          aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name nomad-crew-cluster-production

      - name: Deploy to Kubernetes
        run: |
          kubectl set image deployment/nomad-crew-frontend nomad-crew-frontend=${{ needs.build-and-push.outputs.registry_url }}/${{ env.ECR_REPOSITORY }}:${{ needs.build-and-push.outputs.image_tag }} --record
          kubectl rollout status deployment/nomad-crew-frontend

      - name: Set up rollback capability
        run: |
          CURRENT_REVISION=$(kubectl get deployment/nomad-crew-frontend -o jsonpath='{.metadata.annotations.deployment\.kubernetes\.io/revision}')
          echo "CURRENT_REVISION=$CURRENT_REVISION" >> $GITHUB_ENV

  verify-deployment:
    name: Verify Deployment
    needs: [deploy, build-and-push]
    runs-on: ubuntu-latest

    steps:
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: arn:aws:iam::195275657852:role/NomadCrewProductionDeploymentRole
          aws-region: ${{ env.AWS_REGION }}

      - id: health-check
        name: Check application health
        run: |
          # Get the load balancer DNS
          ALB_DNS=$(aws elbv2 describe-load-balancers --query "LoadBalancers[?contains(LoadBalancerName, 'nomadcrew')].DNSName" --output text)
          echo "Checking application health at https://$ALB_DNS/health/liveness"
          
          # Attempt health check up to 10 times
          for i in {1..10}; do
            HTTP_STATUS=$(curl -s -o /dev/null -w "%{http_code}" https://$ALB_DNS/health/liveness)
            
            if [ "$HTTP_STATUS" -eq 200 ]; then
              echo "Application is healthy!"
              exit 0
            else
              echo "Attempt $i: Application is not healthy yet. Status: $HTTP_STATUS"
              sleep 30
            fi
          done
          
          echo "Application failed to become healthy after multiple attempts."
          exit 1

      - name: Rollback on failure
        if: failure() && steps.health-check.outcome == 'failure'
        run: |
          # Configure kubectl for rollback
          aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name nomad-crew-cluster-production
          
          echo "Health check failed, rolling back deployment..."
          kubectl rollout undo deployment/nomad-crew-frontend
          kubectl rollout status deployment/nomad-crew-frontend
          
          echo "Rollback completed. Please check the application logs for more details."