---
phase: 28-goroutine-management
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - services/notification_worker_pool.go
  - services/notification_worker_pool_test.go
  - config/config.go
autonomous: true

must_haves:
  truths:
    - "Worker pool accepts jobs via Submit() and executes them with bounded concurrency"
    - "Queue has configurable capacity with non-blocking submit (drops when full)"
    - "Workers stop accepting new jobs when context is cancelled"
    - "Shutdown() waits for in-flight jobs with configurable timeout"
    - "Metrics are exposed for queue depth, active workers, dropped jobs"
  artifacts:
    - path: "services/notification_worker_pool.go"
      provides: "Generic worker pool with bounded concurrency"
      exports: ["WorkerPool", "Job", "NewWorkerPool", "WorkerPoolConfig"]
      min_lines: 150
    - path: "services/notification_worker_pool_test.go"
      provides: "Unit tests for worker pool"
      contains: "func Test"
      min_lines: 100
    - path: "config/config.go"
      provides: "Worker pool configuration"
      contains: "WorkerPoolConfig"
  key_links:
    - from: "services/notification_worker_pool.go"
      to: "prometheus/client_golang"
      via: "Prometheus metrics singleton"
      pattern: "prometheus\\.(Gauge|Counter|Histogram)"
    - from: "services/notification_worker_pool.go"
      to: "sync.WaitGroup"
      via: "goroutine tracking"
      pattern: "wg\\.Add|wg\\.Done|wg\\.Wait"
---

<objective>
Create a generic worker pool with bounded concurrency, job queue, and graceful shutdown.

Purpose: Eliminate unbounded goroutine spawning by providing a reusable worker pool that tracks all async work and enables graceful shutdown.

Output: `services/notification_worker_pool.go` with full implementation, tests, and configuration support.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/28-goroutine-management/28-RESEARCH.md

# Existing patterns to follow:
@internal/events/redis_publisher.go (lines 35-125 for metrics singleton, WaitGroup usage)
@config/config.go (for configuration struct patterns)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add WorkerPoolConfig to config/config.go</name>
  <files>config/config.go</files>
  <action>
Add a new WorkerPoolConfig struct and integrate it into the main Config struct.

1. Add the struct after NotificationConfig:
```go
// WorkerPoolConfig holds configuration for the notification worker pool.
type WorkerPoolConfig struct {
    // MaxWorkers is the number of concurrent workers (default: 10)
    MaxWorkers int `mapstructure:"MAX_WORKERS" yaml:"max_workers"`
    // QueueSize is the maximum number of pending jobs (default: 1000)
    QueueSize int `mapstructure:"QUEUE_SIZE" yaml:"queue_size"`
    // ShutdownTimeoutSeconds is the max time to wait for workers during shutdown (default: 30)
    ShutdownTimeoutSeconds int `mapstructure:"SHUTDOWN_TIMEOUT_SECONDS" yaml:"shutdown_timeout_seconds"`
}
```

2. Add to Config struct:
```go
WorkerPool WorkerPoolConfig `mapstructure:"WORKER_POOL" yaml:"worker_pool"`
```

3. Add defaults in LoadConfig():
```go
v.SetDefault("WORKER_POOL.MAX_WORKERS", 10)
v.SetDefault("WORKER_POOL.QUEUE_SIZE", 1000)
v.SetDefault("WORKER_POOL.SHUTDOWN_TIMEOUT_SECONDS", 30)
```

4. Add env bindings:
```go
{"WORKER_POOL.MAX_WORKERS", "WORKER_POOL_MAX_WORKERS"},
{"WORKER_POOL.QUEUE_SIZE", "WORKER_POOL_QUEUE_SIZE"},
{"WORKER_POOL.SHUTDOWN_TIMEOUT_SECONDS", "WORKER_POOL_SHUTDOWN_TIMEOUT_SECONDS"},
```

5. Add validation in validateConfig():
```go
// Validate WorkerPool config
if cfg.WorkerPool.MaxWorkers <= 0 {
    return fmt.Errorf("worker pool max workers must be positive")
}
if cfg.WorkerPool.QueueSize <= 0 {
    return fmt.Errorf("worker pool queue size must be positive")
}
if cfg.WorkerPool.ShutdownTimeoutSeconds <= 0 {
    return fmt.Errorf("worker pool shutdown timeout must be positive")
}
```
  </action>
  <verify>
Run `go build ./config/...` to verify compilation.
Run existing config tests: `go test ./config/...`
  </verify>
  <done>
WorkerPoolConfig struct exists with MaxWorkers, QueueSize, ShutdownTimeoutSeconds fields.
Defaults are 10, 1000, 30 respectively.
Environment variables WORKER_POOL_MAX_WORKERS, WORKER_POOL_QUEUE_SIZE, WORKER_POOL_SHUTDOWN_TIMEOUT_SECONDS are bound.
Validation rejects non-positive values.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create notification_worker_pool.go</name>
  <files>services/notification_worker_pool.go</files>
  <action>
Create the worker pool implementation following the established pattern from `internal/events/redis_publisher.go`.

Key design decisions:
- Use sync.WaitGroup for goroutine tracking (proven pattern in codebase)
- Use buffered channel for job queue
- Non-blocking submit with drop-newest when queue full (simpler than drop-oldest)
- Context-based graceful shutdown
- Prometheus metrics using singleton pattern (avoid double registration)

Structure:
```go
package services

import (
    "context"
    "sync"
    "time"

    "github.com/NomadCrew/nomad-crew-backend/config"
    "github.com/NomadCrew/nomad-crew-backend/logger"
    "github.com/prometheus/client_golang/prometheus"
    "github.com/prometheus/client_golang/prometheus/promauto"
    "go.uber.org/zap"
)

// Job represents a unit of work for the worker pool.
type Job struct {
    Name    string
    Execute func(ctx context.Context) error
}

// WorkerPool manages a bounded set of workers processing jobs from a queue.
type WorkerPool struct {
    jobQueue chan Job
    wg       sync.WaitGroup
    ctx      context.Context
    cancel   context.CancelFunc
    logger   *zap.SugaredLogger
    metrics  *workerPoolMetrics
    config   config.WorkerPoolConfig
    mu       sync.Mutex
    running  bool
}

// workerPoolMetrics holds Prometheus metrics for the worker pool.
type workerPoolMetrics struct {
    queueDepth     prometheus.Gauge
    activeWorkers  prometheus.Gauge
    completedJobs  prometheus.Counter
    droppedJobs    prometheus.Counter
    errorCount     prometheus.Counter
    jobDuration    prometheus.Histogram
}

// Singleton pattern for metrics (avoid double registration)
var (
    wpMetricsInstance *workerPoolMetrics
    wpMetricsOnce     sync.Once
)

func newWorkerPoolMetrics() *workerPoolMetrics {
    wpMetricsOnce.Do(func() {
        wpMetricsInstance = &workerPoolMetrics{
            queueDepth: promauto.NewGauge(prometheus.GaugeOpts{
                Name: "notification_worker_pool_queue_depth",
                Help: "Current number of jobs waiting in queue",
            }),
            activeWorkers: promauto.NewGauge(prometheus.GaugeOpts{
                Name: "notification_worker_pool_active_workers",
                Help: "Current number of workers processing jobs",
            }),
            completedJobs: promauto.NewCounter(prometheus.CounterOpts{
                Name: "notification_worker_pool_completed_jobs_total",
                Help: "Total number of successfully completed jobs",
            }),
            droppedJobs: promauto.NewCounter(prometheus.CounterOpts{
                Name: "notification_worker_pool_dropped_jobs_total",
                Help: "Total number of jobs dropped due to full queue",
            }),
            errorCount: promauto.NewCounter(prometheus.CounterOpts{
                Name: "notification_worker_pool_errors_total",
                Help: "Total number of job execution errors",
            }),
            jobDuration: promauto.NewHistogram(prometheus.HistogramOpts{
                Name:    "notification_worker_pool_job_duration_seconds",
                Help:    "Time taken to execute jobs",
                Buckets: []float64{.01, .05, .1, .25, .5, 1, 2.5, 5, 10},
            }),
        }
    })
    return wpMetricsInstance
}

// NewWorkerPool creates a new worker pool with the given configuration.
func NewWorkerPool(cfg config.WorkerPoolConfig) *WorkerPool {
    ctx, cancel := context.WithCancel(context.Background())
    return &WorkerPool{
        jobQueue: make(chan Job, cfg.QueueSize),
        ctx:      ctx,
        cancel:   cancel,
        logger:   logger.GetLogger().Named("worker-pool"),
        metrics:  newWorkerPoolMetrics(),
        config:   cfg,
    }
}

// Start launches the worker goroutines.
func (wp *WorkerPool) Start() {
    wp.mu.Lock()
    defer wp.mu.Unlock()

    if wp.running {
        wp.logger.Warn("Worker pool already running")
        return
    }
    wp.running = true

    wp.logger.Infow("Starting worker pool",
        "maxWorkers", wp.config.MaxWorkers,
        "queueSize", wp.config.QueueSize)

    for i := 0; i < wp.config.MaxWorkers; i++ {
        wp.wg.Add(1)
        go wp.worker(i)
    }
}

func (wp *WorkerPool) worker(id int) {
    defer wp.wg.Done()
    wp.logger.Debugw("Worker started", "workerId", id)

    for {
        select {
        case <-wp.ctx.Done():
            wp.logger.Debugw("Worker stopping (context cancelled)", "workerId", id)
            return
        case job, ok := <-wp.jobQueue:
            if !ok {
                wp.logger.Debugw("Worker stopping (channel closed)", "workerId", id)
                return
            }
            wp.executeJob(id, job)
        }
    }
}

func (wp *WorkerPool) executeJob(workerID int, job Job) {
    wp.metrics.activeWorkers.Inc()
    wp.metrics.queueDepth.Dec()

    start := time.Now()

    // Create a context with timeout for the job
    jobCtx, cancel := context.WithTimeout(wp.ctx, 30*time.Second)
    defer cancel()

    if err := job.Execute(jobCtx); err != nil {
        wp.logger.Errorw("Job execution failed",
            "job", job.Name,
            "workerId", workerID,
            "error", err,
            "duration", time.Since(start))
        wp.metrics.errorCount.Inc()
    } else {
        wp.logger.Debugw("Job completed",
            "job", job.Name,
            "workerId", workerID,
            "duration", time.Since(start))
    }

    wp.metrics.jobDuration.Observe(time.Since(start).Seconds())
    wp.metrics.completedJobs.Inc()
    wp.metrics.activeWorkers.Dec()
}

// Submit adds a job to the queue. Returns true if queued, false if dropped.
func (wp *WorkerPool) Submit(job Job) bool {
    select {
    case wp.jobQueue <- job:
        wp.metrics.queueDepth.Inc()
        wp.logger.Debugw("Job submitted", "job", job.Name)
        return true
    default:
        wp.metrics.droppedJobs.Inc()
        wp.logger.Warnw("Job dropped - queue full",
            "job", job.Name,
            "queueSize", wp.config.QueueSize)
        return false
    }
}

// Shutdown gracefully stops the worker pool, waiting for in-flight jobs.
func (wp *WorkerPool) Shutdown(ctx context.Context) error {
    wp.mu.Lock()
    if !wp.running {
        wp.mu.Unlock()
        return nil
    }
    wp.running = false
    wp.mu.Unlock()

    wp.logger.Info("Initiating worker pool shutdown...")

    // Signal workers to stop accepting new jobs from context
    wp.cancel()

    // Close channel so workers exit their loops after draining
    close(wp.jobQueue)

    // Wait for workers with timeout
    done := make(chan struct{})
    go func() {
        wp.wg.Wait()
        close(done)
    }()

    select {
    case <-done:
        wp.logger.Info("Worker pool shutdown complete - all workers finished")
        return nil
    case <-ctx.Done():
        wp.logger.Warn("Worker pool shutdown timed out - some workers may still be running")
        return ctx.Err()
    }
}

// QueueDepth returns the current number of jobs waiting in the queue.
func (wp *WorkerPool) QueueDepth() int {
    return len(wp.jobQueue)
}

// IsRunning returns whether the worker pool is currently running.
func (wp *WorkerPool) IsRunning() bool {
    wp.mu.Lock()
    defer wp.mu.Unlock()
    return wp.running
}
```

Important patterns from codebase:
- Use `logger.GetLogger().Named()` for namespaced logging
- Use `promauto.NewXxx` for automatic metric registration
- Use `sync.Once` for singleton metrics (prevents test conflicts)
- Always `wg.Add(1)` BEFORE `go func()`, `defer wg.Done()` at start
  </action>
  <verify>
Run `go build ./services/notification_worker_pool.go` to verify compilation.
Run `go vet ./services/...` to check for issues.
  </verify>
  <done>
WorkerPool struct exists with Start(), Submit(), Shutdown(), QueueDepth(), IsRunning() methods.
Prometheus metrics are registered: queue_depth, active_workers, completed_jobs_total, dropped_jobs_total, errors_total, job_duration_seconds.
All metrics use singleton pattern to avoid double registration.
  </done>
</task>

<task type="auto">
  <name>Task 3: Create notification_worker_pool_test.go</name>
  <files>services/notification_worker_pool_test.go</files>
  <action>
Create comprehensive tests for the worker pool.

Test cases:
1. TestWorkerPool_SubmitAndExecute - Basic job submission and execution
2. TestWorkerPool_BoundedConcurrency - Verify only maxWorkers run concurrently
3. TestWorkerPool_QueueFull - Verify jobs are dropped when queue full
4. TestWorkerPool_GracefulShutdown - Verify in-flight jobs complete
5. TestWorkerPool_ShutdownTimeout - Verify timeout is respected
6. TestWorkerPool_DoubleStart - Verify idempotent Start()
7. TestWorkerPool_JobError - Verify errors are logged but don't crash workers

```go
package services

import (
    "context"
    "sync"
    "sync/atomic"
    "testing"
    "time"

    "github.com/NomadCrew/nomad-crew-backend/config"
    "github.com/stretchr/testify/assert"
    "github.com/stretchr/testify/require"
)

func TestWorkerPool_SubmitAndExecute(t *testing.T) {
    cfg := config.WorkerPoolConfig{
        MaxWorkers:             2,
        QueueSize:              10,
        ShutdownTimeoutSeconds: 5,
    }

    pool := NewWorkerPool(cfg)
    pool.Start()
    defer pool.Shutdown(context.Background())

    var executed int32
    done := make(chan struct{})

    submitted := pool.Submit(Job{
        Name: "test-job",
        Execute: func(ctx context.Context) error {
            atomic.AddInt32(&executed, 1)
            close(done)
            return nil
        },
    })

    require.True(t, submitted, "Job should be accepted")

    select {
    case <-done:
        // Job completed
    case <-time.After(2 * time.Second):
        t.Fatal("Job did not execute within timeout")
    }

    assert.Equal(t, int32(1), atomic.LoadInt32(&executed))
}

func TestWorkerPool_BoundedConcurrency(t *testing.T) {
    cfg := config.WorkerPoolConfig{
        MaxWorkers:             2,
        QueueSize:              100,
        ShutdownTimeoutSeconds: 5,
    }

    pool := NewWorkerPool(cfg)
    pool.Start()
    defer pool.Shutdown(context.Background())

    var maxConcurrent int32
    var currentConcurrent int32
    var mu sync.Mutex

    var wg sync.WaitGroup
    for i := 0; i < 10; i++ {
        wg.Add(1)
        pool.Submit(Job{
            Name: "concurrent-job",
            Execute: func(ctx context.Context) error {
                defer wg.Done()

                current := atomic.AddInt32(&currentConcurrent, 1)
                defer atomic.AddInt32(&currentConcurrent, -1)

                mu.Lock()
                if current > maxConcurrent {
                    maxConcurrent = current
                }
                mu.Unlock()

                time.Sleep(50 * time.Millisecond)
                return nil
            },
        })
    }

    wg.Wait()

    assert.LessOrEqual(t, maxConcurrent, int32(2), "Should never exceed 2 concurrent workers")
}

func TestWorkerPool_QueueFull(t *testing.T) {
    cfg := config.WorkerPoolConfig{
        MaxWorkers:             1,
        QueueSize:              2,
        ShutdownTimeoutSeconds: 5,
    }

    pool := NewWorkerPool(cfg)
    pool.Start()
    defer pool.Shutdown(context.Background())

    // Block the worker
    blocker := make(chan struct{})
    pool.Submit(Job{
        Name: "blocker",
        Execute: func(ctx context.Context) error {
            <-blocker
            return nil
        },
    })

    // Fill the queue
    time.Sleep(10 * time.Millisecond) // Let worker pick up blocker
    pool.Submit(Job{Name: "queued-1", Execute: func(ctx context.Context) error { return nil }})
    pool.Submit(Job{Name: "queued-2", Execute: func(ctx context.Context) error { return nil }})

    // This should be dropped
    dropped := !pool.Submit(Job{Name: "overflow", Execute: func(ctx context.Context) error { return nil }})
    assert.True(t, dropped, "Job should be dropped when queue is full")

    close(blocker)
}

func TestWorkerPool_GracefulShutdown(t *testing.T) {
    cfg := config.WorkerPoolConfig{
        MaxWorkers:             2,
        QueueSize:              10,
        ShutdownTimeoutSeconds: 5,
    }

    pool := NewWorkerPool(cfg)
    pool.Start()

    var completed int32

    // Submit a slow job
    pool.Submit(Job{
        Name: "slow-job",
        Execute: func(ctx context.Context) error {
            time.Sleep(100 * time.Millisecond)
            atomic.AddInt32(&completed, 1)
            return nil
        },
    })

    // Give time for job to be picked up
    time.Sleep(10 * time.Millisecond)

    // Shutdown should wait for the job
    ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second)
    defer cancel()

    err := pool.Shutdown(ctx)
    require.NoError(t, err)

    assert.Equal(t, int32(1), atomic.LoadInt32(&completed), "Job should complete during shutdown")
}

func TestWorkerPool_ShutdownTimeout(t *testing.T) {
    cfg := config.WorkerPoolConfig{
        MaxWorkers:             1,
        QueueSize:              10,
        ShutdownTimeoutSeconds: 1,
    }

    pool := NewWorkerPool(cfg)
    pool.Start()

    // Submit a job that takes longer than timeout
    pool.Submit(Job{
        Name: "very-slow-job",
        Execute: func(ctx context.Context) error {
            select {
            case <-ctx.Done():
                return ctx.Err()
            case <-time.After(10 * time.Second):
                return nil
            }
        },
    })

    // Give time for job to be picked up
    time.Sleep(10 * time.Millisecond)

    ctx, cancel := context.WithTimeout(context.Background(), 100*time.Millisecond)
    defer cancel()

    err := pool.Shutdown(ctx)
    assert.Error(t, err, "Shutdown should timeout")
    assert.Equal(t, context.DeadlineExceeded, err)
}

func TestWorkerPool_DoubleStart(t *testing.T) {
    cfg := config.WorkerPoolConfig{
        MaxWorkers:             2,
        QueueSize:              10,
        ShutdownTimeoutSeconds: 5,
    }

    pool := NewWorkerPool(cfg)
    pool.Start()
    pool.Start() // Should be idempotent
    defer pool.Shutdown(context.Background())

    assert.True(t, pool.IsRunning())
}

func TestWorkerPool_JobError(t *testing.T) {
    cfg := config.WorkerPoolConfig{
        MaxWorkers:             1,
        QueueSize:              10,
        ShutdownTimeoutSeconds: 5,
    }

    pool := NewWorkerPool(cfg)
    pool.Start()
    defer pool.Shutdown(context.Background())

    var executed int32

    // First job errors
    pool.Submit(Job{
        Name: "error-job",
        Execute: func(ctx context.Context) error {
            atomic.AddInt32(&executed, 1)
            return assert.AnError
        },
    })

    // Second job should still run
    done := make(chan struct{})
    pool.Submit(Job{
        Name: "success-job",
        Execute: func(ctx context.Context) error {
            atomic.AddInt32(&executed, 1)
            close(done)
            return nil
        },
    })

    select {
    case <-done:
        // Good
    case <-time.After(2 * time.Second):
        t.Fatal("Second job did not execute")
    }

    assert.Equal(t, int32(2), atomic.LoadInt32(&executed), "Both jobs should execute")
}
```
  </action>
  <verify>
Run `go test ./services/notification_worker_pool_test.go -v` to verify all tests pass.
Run `go test -race ./services/notification_worker_pool_test.go` to check for data races.
  </verify>
  <done>
All 7 test cases pass.
No data races detected.
Tests verify: basic execution, bounded concurrency, queue overflow, graceful shutdown, timeout, idempotency, error handling.
  </done>
</task>

</tasks>

<verification>
1. `go build ./config/...` - Config compiles
2. `go build ./services/notification_worker_pool.go` - Worker pool compiles
3. `go test ./services/notification_worker_pool_test.go -v` - All tests pass
4. `go test -race ./services/notification_worker_pool_test.go` - No data races
5. `go vet ./services/...` - No issues
</verification>

<success_criteria>
1. WorkerPoolConfig exists in config/config.go with MaxWorkers (default 10), QueueSize (default 1000), ShutdownTimeoutSeconds (default 30)
2. Environment variables WORKER_POOL_MAX_WORKERS, WORKER_POOL_QUEUE_SIZE, WORKER_POOL_SHUTDOWN_TIMEOUT_SECONDS are functional
3. WorkerPool has Start(), Submit(), Shutdown(), QueueDepth(), IsRunning() methods
4. Prometheus metrics exposed: notification_worker_pool_queue_depth, notification_worker_pool_active_workers, notification_worker_pool_completed_jobs_total, notification_worker_pool_dropped_jobs_total, notification_worker_pool_errors_total, notification_worker_pool_job_duration_seconds
5. All 7 tests pass with no data races
</success_criteria>

<output>
After completion, create `.planning/phases/28-goroutine-management/28-01-SUMMARY.md`
</output>
